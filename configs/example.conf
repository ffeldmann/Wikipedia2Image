# All paths to different required data objects
images_dir: "data/wikidata_politicans/all/images"
data_path: "data/wikidata_politicans/all/"
data_path_val: "data/wikidata_politicans/val"
val_images_dir: "data/wikidata_politicans/val"
pretrained_encoder_dir: "tf-hub_modules/text_encoder/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47"
pretrained_encoder_file: "infersent2.pkl"
pretrained_embedding_file: "glove.840B.300d.txt"
log_dir: "training_runs/example/losses/"
sample_dir: "training_runs/example/generated_samples/"
save_dir: "training_runs/example/saved_models/"

# tensorboard comment
tensorboard_comment: "example"

# Hyperparameters for the Model
captions_length: 100
img_dims:
  - 256
  - 256

# whether to use a pretrained encoder:
download_pretrained_encoder: False
use_pretrained_encoder: True
p_proc_gpu_mem: 0.3

# LSTM hyperparameters
embedding_size: 128
hidden_size: 512
num_layers: 3  # number of LSTM cells in the encoder network

# Conditioning Augmentation hyperparameters
hidden_size: 4096  # output size of the Pretrained encoder
ca_out_size: 256

compressed_latent_size: 128

# Pro GAN hyperparameters
use_eql: True
use_ema: True
ema_decay: 0.999
depth: 7
latent_size: 512
learning_rate: 0.001
beta_1: 0
beta_2: 0.99
eps: 0.00000001
drift: 0.001
n_critic: 1

# Training hyperparameters:
epochs:
  - 5
  - 10
  - 40
  - 40
  - 10 
  - 20
  - 40

# % of epochs for fading in the new layer
fade_in_percentage:
  - 50
  - 50
  - 50
  - 50
  - 50
  - 50
  - 50

batch_sizes:
  - 128
  - 128 
  - 64 
  - 64 
  - 16 
  - 8 
  - 8 

loss_function: "wgan-gp"  # the loss function to be used
num_workers: 8 
feedback_factor: 7  # number of logs generated per epoch
checkpoint_factor: 1  # save the models after these many epochs
use_matching_aware_discriminator: False  # use the matching aware discriminator
